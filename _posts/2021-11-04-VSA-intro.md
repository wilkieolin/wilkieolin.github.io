---
layout: post
title: A Brief Introduction to Neuromorphic Computing
categories: computing research
---

The question "what is neuromorphic computing" has a textbook answer: it's research in computing methods which are inspired by the brain. However, the problem with this definiation is that because there are so many ways be inspired by the brain, it often appears debatable whether a certain approach is neuromorphic. 

For instance, as their name suggests, the first neural networks were inspired by rate-based interpretations of a biological neuron's firing. And yet, it is rare that these highly-successful networks or their hardware accelerators are referred to as 'neuromorphic.' 

From this outlook, it might appear that what is declared a neuromorphic architecture only reflects on the intent of a researcher to place a work within a certain field or market it to a specific audience. While there may be some truth to that, in my experience there are clear markers of what a constitutes a modern neuromorphic strategy: a focus on utilizing forms of sparsity, achieving distributed computing, and keeping an eye towards novel hardware implementations. These principles just take a bit more explaining, so let's get into it now.

<!--more-->

# The Motivation for Neuromorphic

Biological computation is highly efficient, both in terms of energy usage and in learning new information; the human brain uses only [tens of watts](https://www.scientificamerican.com/article/thinking-hard-calories/) and is capable of flexibly learning new tasks. This contrasts greatly to modern artificial intelligence (AI) models, which [consume vast amounts of energy to train](https://arxiv.org/abs/1906.02243) and are still faced with the challenge of '[catastrophic forgetting](https://www.sciencedirect.com/science/article/pii/S0893608019300231),' meaning they cannot easily learn new situations or apply their existing knowledge to new tasks.

For example, if a purely hypothetical game 'Starcraft 3' were released tomorrow, I would be confident I could play through its campaign on a normal difficulty level by applying my experience with its predecessor, Starcraft 2. New units or graphics would doubtless be included with the new entry, but I would be able to incorporate these new elements as I played.

In contrast, it is likely that the AI models such as [AlphaStar](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) currently used to play Starcraft 2 would require moderate or extensive reworking to incorporate these new elements, as well as re-training. 

While this inefficiency might not be a problem for well-defined applications with huge amounts of data available, in other areas where AI could be deployed it's a deal-breaker. [Robotics is one key example](https://spectrum.ieee.org/how-deepmind-is-reinventing-the-robot): a robotic 'agent' deployed in a field needs to be able to reason about new objects and scenarios it hasn't encountered before, and do so safely and reliably. Furthermore, it needs to be able to do this without a cable attaching it to a power plant and/or supercomputer. 

AI faces another challenge: historically, its progress has been [linked to an increasing number of model parameters](https://dl.acm.org/doi/pdf/10.1145/3381831), where each parameter is essentialy a 'dial' which needs to be turned to the correct position for the model to produce the right answer. Networks which revolutionized image classification in 2013 used [millions of parameters](https://towardsdatascience.com/understanding-alexnet-a-detailed-walkthrough-20cd68a490aa); in 2021, networks focused on advancing 'natural language processing' (NLP) tasks such as translation use [billions](https://developer.nvidia.com/blog/openai-presents-gpt-3-a-175-billion-parameters-language-model/) or even [trillions](https://arxiv.org/pdf/2101.03961.pdf) of parameters. 

Needing to tune this many parameters places designing and training these models outside the abilities of any but the largest corporations and governments with sufficient resources. Furthermore, the slowdown in transistor scaling which provided faster and cheaper computers for decades makes it likely that these models will remain a challenge to train and deploy without [specialized hardware](https://www.hpcwire.com/2021/05/20/google-launches-tpu-v4-ai-chips/). 

Besides these, other problems exist with modern AI such as [bias](https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai), [lack of robustness](https://towardsdatascience.com/breaking-neural-networks-with-adversarial-attacks-f4290a9a45aa), and [lack of explainability](https://en.wikipedia.org/wiki/Explainable_artificial_intelligence). To realize the full potential of AI as reliable, autonomous systems which can improve human quality-of-life in areas such as medicine, manual labor, and household assistance, these issues must be addressed. 

While resolving all these issues may seem insurmountable, we know it must be possible, as the human brain is capable - to a certain extent - of addressing these issues. In the current era of AI, neuromorphic computing seeks to these extant issues by applying principles of biological computation. Here, I'll focus on two broad topics which neuromorphic researchers often aim to achieve: distribution and sparsity. 

# Distribution

Compared to a computer chip, the human brain is remarkably resilient. Individual sections of the brain can be damaged or removed, but after a recovery period overall activity recovers and a patient can return to a more normal lifestyle. In contrast, computers are fragile; making a small, random cut across a processor would almost certainly result in it failing. 

If you have a task which is very important to complete, you can take an alternate approach from executing it on one computer: instead, you can send the same task to multiple computers and examine the answers you get back. If the majority of the answers are the same, you can assume that answer is correct and use it. This way, even if individual parts of the system fail, the overall process can overcome those failures. This is a very simple method for what's known as 'distributed computing,' but the principles remain the same: using networks of components that pass messages, creating an overall system which can correctly carry out its task even when individual components fail.

It's hypothesized that in many ways, the processing the brain carries out is distributed. Certain components (neurons and synapses) can fail, but the overall computation remains the same. 

# Sparsity